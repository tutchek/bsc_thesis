\chapter{Benchmark results}
\label{results}

In this chapter we present the results of the performance measuring of each solver.
There are two types of tests. The robustness test and the performance test. In the robustness
test we compare the solvers according to the size of the task which the solver
was able to compute in a given time limit. In the performance tests we measure the time 
which is needed to find a solution to the problem and the consumed memory. We used 
for this purpose designed tool. The tool runs the solver executable with given parameters.
While the solver process is running the benchmarking tool periodically checks the \texttt{/proc/(pid)/stat} file
and saves the current state. If the elapsed time is larger than the given limit the 
\texttt{SIGINT} signal is sent to the process. The process has one second to end
and produce some output. If the process is still running one second after the 
\texttt{SIGINT} signal the \texttt{SIGKILL} signal is sent and process is killed. 
In the robustness test the tool tries to estimate the size of the task using the
binary search.

All of the benchmarks were performed on a single dedicated computer Pentium IV, 3GHz (single core 
with hyper threading),
1GB RAM running the Debian Linux with Linux kernel 2.6.18. Except the Mozart/Oz 
solver and SICStus Prolog all solvers were compiled on the target machine. The 
precompiled binaries distribution was used for the Mozart/Oz solver because the
source distribution contained in the time of writing of this thesis error which
prevented compiling of the code. The SICStus Prolog is not distributed as source 
code but only as a binary. Therefore we used the shipped executable. Since the 
SICStus Prolog is not a free open-source software we used a trial version of the solver.
There should be limited only the period when the solver can be used. The performance
of the solver should not be affected.

The following versions of the solvers were used:
\begin{itemize}
  \item Mozart 1.4.0-20080704
  \item Choco 2.1.0
  \item Minion 0.8.1
  \item Gecode 3.1.0
  \item \eclipse 6.0\_96
  \item SICStus Prolog 4.0.7 -- Linux, glibc 2.7
\end{itemize}   

In the performance test as well as in the robustness test we used the fist-fail search strategy for
all of the implemented benchmarks. This strategy is the default strategy the all
solvers.

\section{The robustness test}
In the robustness test we measure how long magic sequence the solver is able to compute in 
ten minutes. As stated in the previous paragraph the benchmarking tool uses for this 
purpose the binary search. The tool has an interval $[a,b]$ of values on which the search 
is performed. It picks a value $v$ in the middle of the interval and tries to compute
the solution of that length. If it success it changes the interval to $[v+1,b]$, if 
it fails it changes the interval to $[a,v-1]$. Then the search continues in the same way
until the lower bound of the interval is larger than upper bound. The results of
the robustness test are in the table \ref{results:robustness}.

The results of the robustness test are in the table \ref{results:robustness} (the best result is bolded). As 
we can see the most robust solver is the Minion solver followed by the Gecode.
At first we tried to implement the benchmark problem in the Gecode system by
using of the constraint \texttt{count}. This constraint meaning is exactly the
Magic Sequence problem; however, the longest magic sequence which we were able to compute
had the length 27. If we compare this value with the current value 191 it is obvious
that the implementation of this constraint is problematic.  

\begin{table}
\caption{\label{results:robustness}The results of the robustness test.}
\begin{center}
\begin{tabular}{cccccc}
\hline Mozart/Oz & Choco & Minion & Gecode & \eclipse & SICStus Prolog \\
\hline
  94 & 111 & {\bf 236} & 191 & 55 & 174 \\
\hline 
 % Gecode puvodne 27
\end{tabular}
\end{center}
\end{table}

\section{The performance test}
This test compares the solvers according to the two criteria. The first criterion is
the time needed to solve given problem instances. The second criterion is the highest
peak of used computer memory during the computation. We divided the instances of the benchmarks into two groups. 
In the first group there are the instances of the Quasigroup With Holes problem. 

We generated 25 balanced quasigroups of the 
order $n = 16$ and removed approximately $1.6 n^{1.55}$ values. According to the 
\cite{Achlioptas00generatingsatisfiable} the quasigroups with this number of the 
holes are hard to solve compared to the other quasigroup of the same order. To 
generate the quasigroups we used the {\em lsencode} combined with the {\em walksat}.
 
This benchmark gives us a comparison of the \texttt{allDifferent} constraint. This 
constraint is a global constraint. That means that the constraint is not binary but
it constrain a set of variables. The allDifferent constraint is typically implemented
using the matching in a bipartite graph \cite{hoeve:alldifferent}. The one group of nodes consists of the 
variables and the second group consists of the possible values of these variables.
The variable node and the value node are connected by the edge if and only if
the value is in the variables' domain. 

As we can see in the graph in the figure \ref{results:qwh1} (a version with logarithmic 
scale is in the figure \ref{results:qwh2}) the best results were achieved with Gecode,
Mozart and Minion. The performance of the SICStus Prolog and \eclipse compared to
the others is worse than the order of the magnitude.
  
In the second group of benchmarks there are the following instances of the benchmarks:

\begin{itemize}
  \item {\em queens10} -- find all solutions of the 10 queens problem,
  \item {\em queens100} --find one solution of the 100 queens problem, 
  \item {\em magic20} -- find the Magic sequence of the length 20,
  \item {\em srq} -- find the solution of the Self Referential Quiz,
  \item {\em warehouses} -- find the best solution of the Locating Warehouses problem.
\end{itemize}


\begin{figure}
  \centering
  \includegraphics[width=.5\textwidth, angle=270]{images/grafy/16.time.nolog.ps}
  \caption{The QWH benchmark results}
  \label{results:qwh1}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=.5\textwidth, angle=270]{images/grafy/16.time.ps}
  \caption{The QWH benchmark results, logarithmic scale}
  \label{results:qwh2}
\end{figure}

As we can see in the tables \ref{results:performance:runtime} and \ref{results:performance:memory} (the best result of each benchmark is bolded) none 
of the solvers is the ultimate winner; however, the three solvers -- SICStus Prolog, Gecode and 
Mozart -- achieved the best results in some of the problem instances. \eclipse and
Minion did not solve the 100 queens problem in the given time limit. Therefore we 
consider them as unreliable. The \eclipse and Choco were unable to load the big problems
to the memory so we changed the memory limit to one gigabyte. For the \eclipse we changed it for all of the
experiments and for the Choco we used this large memory only for the magic sequence problem.
The table \ref{results:performance:memory} indicates that the Java Virtual Machine which
is running the Choco as well as the \eclipse solver allocated the whole allowed
amount of memory. We observer in the performance test the same problem with the \texttt{count}
constraint in the Gecode system as in the robustness test. For a comparison the original
results for the magic20 benchmark was 2926.86 milliseconds. The memory peak was nearly the same.

% gecode puvodne 2926.86ms    
\begin{table}
\caption{\label{results:performance:runtime}The results of the performance test. Runtime (ms)}
\begin{center}
\begin{tabular}{lrrrrrr}
\hline Benchmark & Moz & Cho & Min & Gec & ECL & SiP \\
\hline queens10 & 237.95 & 850.27 & 227.69 & {\bf 65.35} & 809.45 & 327.53 \\
	queens100 & 184.19 & 1005.02 & $\times$ & 34.70 & $\times$ & {\bf 15.40} \\
	magic20 & {\bf 11.48} & 1092.95 & 71.24 & 65.41 & 4514.14 & 198.32 \\
	srq & {\bf 6.22} & 482.81 & 55.81 & 25.89 & 691.13 & 13.54 \\
	warehouses & 68.28 & 1500.83 & 2338.45 & 169.71 & 1808.29 & {\bf 14.41} \\  
\hline 
\end{tabular}
\end{center}
\end{table}

\begin{table}
\caption{\label{results:performance:memory}The results of the performance test. Memory peak (MB)}
\begin{center}
\begin{tabular}{lrrrrrr}
\hline Benchmark & Moz & Cho & Min & Gec & ECL & SiP \\
\hline 	queens10 & 8.62 & 209.42 & 44.17 & {\bf 8.58} & 1158.15 & 38.08 \\
				queens100 & 23.31 & 209.46 & $\times$ & 6.71 & $\times$ & {\bf 1.35} \\
				magic20 & {\bf 2.32} & 1173.19 & 44.57 & 7.26 & 1157.99 & 39.30 \\
				srq & 63.36 & 209.35 & 44.56 & 6.68 & 1158.08 & {\bf 0.94} \\
				warehouses & 6.79  & 209.12 & 44.30 & 7.00 & 1157.99 & {\bf 0.89} \\
\hline
\end{tabular}
\end{center}
\end{table}
